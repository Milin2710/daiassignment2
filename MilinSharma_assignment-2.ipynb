{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be55d966-e39b-4492-b4e7-cf7b5b47818e",
   "metadata": {},
   "source": [
    "## ***MILIN SHARMA 23118045***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0c2de7-8cdf-4975-a54f-bb50e91ddaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score,  accuracy_score\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "from statsmodels.stats.diagnostic import linear_rainbow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd6f6ba-a37f-4a19-8a02-949cbec0a1b7",
   "metadata": {},
   "source": [
    "## Reading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d8476b-8e49-4627-a950-3becd11750bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"tips (data for regression problem).csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be9491f-b847-4789-89a9-03c2e6ffe5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa653560-8db4-4542-a6a8-0b6d010afebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e1c31a-5205-4cf6-b2c5-db0ad57cfee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5fd822-e8d4-4a7c-8443-0f1712b1e9c3",
   "metadata": {},
   "source": [
    "# Extracting numerical columns into a separate dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d21338-f048-4955-ad24-76e228ffb023",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_n = df.select_dtypes(include=['number'])\n",
    "print(df_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce983f3e-5184-48f7-acc4-2dca551ebe4a",
   "metadata": {},
   "source": [
    "## Converting the categorical columns into numerical using one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d01d90-7a2c-4b99-a265-c125d8102f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, drop_first=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72afe65f-1012-4734-a4af-a5619df57b7a",
   "metadata": {},
   "source": [
    "## Plotting various graphs to analyze the relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a965d4b8-05a6-4270-a1d9-c7ee7d159b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Scatter Plot Visualization\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.scatterplot(x='amount_billed', y='gratuity', data=df)\n",
    "plt.title('Relationship between Bill and Tip Amount')\n",
    "plt.xlabel('Amount of Bill')\n",
    "plt.ylabel('Amount of Tip')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c873cc-8c52-438d-8496-e3ac830e07d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Pair Plot\n",
    "sns.pairplot(df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d4acc3-c3f6-488c-82e0-d82d91ce46a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Correlation Matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(df.corr(), annot=True, cmap='Spectral')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87fab59-b800-4d0d-b0eb-6aceaabec7de",
   "metadata": {},
   "source": [
    "##  for Linearity - Rainbow Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03862a7-dd5c-4bea-a465-80fff88e0c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data and scale features\n",
    "X_data = df.drop(columns='tip')\n",
    "y_target = df['tip']\n",
    "scaler = StandardScaler()\n",
    "X_scaled_data = scaler.fit_transform(X_data)\n",
    "X_scaled_data = sm.add_constant(X_scaled_data)\n",
    "\n",
    "# Fit OLS model\n",
    "model_ols = sm.OLS(y_target, X_scaled_data).fit()\n",
    "\n",
    "# Perform the Rainbow Test for model linearity\n",
    "statistic, p_value = linear_rainbow(model_ols)\n",
    "print(f\"Rainbow Test Statistic: {statistic}\")\n",
    "print(f\"p-value: {p_value}\")\n",
    "\n",
    "if p_value > 0.05:\n",
    "    print(\"No evidence against linearity (p-value > 0.05). Linearity assumption holds.\")\n",
    "else:\n",
    "    print(\"Evidence against linearity (p-value <= 0.05). Non-linear models may be needed.\")\n",
    "\n",
    "# Predict values\n",
    "predicted_values = model_ols.predict(X_scaled_data)\n",
    "\n",
    "# Plot Actual vs Predicted values\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=y_target, y=predicted_values)\n",
    "plt.plot([y_target.min(), y_target.max()], [y_target.min(), y_target.max()], color='red', linestyle='--', lw=2)  # 45-degree line\n",
    "plt.xlabel(\"Observed Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Comparison of Actual and Predicted Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe51326-9d1d-42a9-833f-55bc350be315",
   "metadata": {},
   "source": [
    "## Residuals plot to check for linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f207b17-5abb-4305-9acd-6e41a717ca3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a linear regression model\n",
    "reg_model = LinearRegression()\n",
    "reg_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predicted_values_test = reg_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate the residuals\n",
    "residuals_test = y_test - predicted_values_test\n",
    "\n",
    "# Plot the residuals\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.residplot(x=predicted_values_test, y=residuals_test, lowess=True, line_kws={'color': 'red', 'lw': 2})\n",
    "plt.xlabel(\"Test Set Predicted Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Residuals Plot to Assess Linearity\")\n",
    "plt.axhline(0, color='black', linestyle='--', lw=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d519e1fb-220b-4093-b036-0e9d95584f9c",
   "metadata": {},
   "source": [
    "## Line Plot (for time-series data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd142c0-ad47-4ab4-9b86-60eae3aa43f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort data by the total bill (used as a proxy for time series)\n",
    "sorted_df = df.sort_values(by='total_bill')\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(sorted_df['total_bill'], sorted_df['tip'], marker='o', linestyle='-', color='b')\n",
    "plt.title(\"Tip Amounts vs. Total Bill\")\n",
    "plt.xlabel(\"Bill Amount\")\n",
    "plt.ylabel(\"Tip Given\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb631d99-6545-42a3-be3b-fac9a6f92607",
   "metadata": {},
   "source": [
    "## **Performing regression tests to predict tip amount using all the features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b66ddf-a36d-439e-be7d-f1b4a7b122d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model Training and Performance Evaluation\n",
    "evaluation_results = {}\n",
    "\n",
    "# 1. Linear Regression\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train, y_train)\n",
    "y_pred_linear = linear_model.predict(X_test)\n",
    "evaluation_results['Linear Regression'] = {\n",
    "    'MSE': mean_squared_error(y_test, y_pred_linear),\n",
    "    'R2 Score': r2_score(y_test, y_pred_linear)\n",
    "}\n",
    "\n",
    "# 2. Ridge Regression\n",
    "ridge_regressor = Ridge(alpha=1.0)\n",
    "ridge_regressor.fit(X_train, y_train)\n",
    "y_pred_ridge = ridge_regressor.predict(X_test)\n",
    "evaluation_results['Ridge Regression'] = {\n",
    "    'MSE': mean_squared_error(y_test, y_pred_ridge),\n",
    "    'R2 Score': r2_score(y_test, y_pred_ridge)\n",
    "}\n",
    "\n",
    "# 3. Lasso Regression\n",
    "lasso_regressor = Lasso(alpha=0.1)\n",
    "lasso_regressor.fit(X_train, y_train)\n",
    "y_pred_lasso = lasso_regressor.predict(X_test)\n",
    "evaluation_results['Lasso Regression'] = {\n",
    "    'MSE': mean_squared_error(y_test, y_pred_lasso),\n",
    "    'R2 Score': r2_score(y_test, y_pred_lasso)\n",
    "}\n",
    "\n",
    "# 4. Decision Tree Regressor\n",
    "decision_tree_model = DecisionTreeRegressor()\n",
    "decision_tree_model.fit(X_train, y_train)\n",
    "y_pred_dt = decision_tree_model.predict(X_test)\n",
    "evaluation_results['Decision Tree Regression'] = {\n",
    "    'MSE': mean_squared_error(y_test, y_pred_dt),\n",
    "    'R2 Score': r2_score(y_test, y_pred_dt)\n",
    "}\n",
    "\n",
    "# 5. Random Forest Regressor\n",
    "random_forest_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "random_forest_model.fit(X_train, y_train)\n",
    "y_pred_rf = random_forest_model.predict(X_test)\n",
    "evaluation_results['Random Forest Regression'] = {\n",
    "    'MSE': mean_squared_error(y_test, y_pred_rf),\n",
    "    'R2 Score': r2_score(y_test, y_pred_rf)\n",
    "}\n",
    "\n",
    "# 6. Support Vector Regression (SVR)\n",
    "svr_regressor = SVR()\n",
    "svr_regressor.fit(X_train, y_train)\n",
    "y_pred_svr = svr_regressor.predict(X_test)\n",
    "evaluation_results['Support Vector Regression'] = {\n",
    "    'MSE': mean_squared_error(y_test, y_pred_svr),\n",
    "    'R2 Score': r2_score(y_test, y_pred_svr)\n",
    "}\n",
    "\n",
    "# 7. K-Nearest Neighbors (KNN) Regression\n",
    "knn_regressor = KNeighborsRegressor(n_neighbors=5)\n",
    "knn_regressor.fit(X_train, y_train)\n",
    "y_pred_knn = knn_regressor.predict(X_test)\n",
    "evaluation_results['K-Nearest Neighbors Regression'] = {\n",
    "    'MSE': mean_squared_error(y_test, y_pred_knn),\n",
    "    'R2 Score': r2_score(y_test, y_pred_knn)\n",
    "}\n",
    "\n",
    "# Print evaluation results\n",
    "for model_name, metrics in evaluation_results.items():\n",
    "    print(f\"{model_name}: MSE = {metrics['MSE']:.2f}, R2 Score = {metrics['R2 Score']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c1aed8-fedd-44a9-a2c3-5ad9c359a2f3",
   "metadata": {},
   "source": [
    "### SVR seems to be the most promising model, followed by Lasso Regression because of low MSE and high R2 Score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7da06f-813f-4543-8344-c6f503236c34",
   "metadata": {},
   "source": [
    "### **Next we can perform feature enginnering to enhance the accuracy of model by selecting different types of columns**\n",
    "### -   Using only numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6aea15-3893-4e27-84df-0acf26cb69e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into features and target variable\n",
    "X = df_n.drop('tip', axis=1)  \n",
    "y = df_n['tip']               \n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d92c7f1-abe6-4b39-bf26-1a3361344a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model Training and Performance Evaluation\n",
    "model_results = {}\n",
    "\n",
    "# 1. Linear Regression\n",
    "linear_regressor = LinearRegression()\n",
    "linear_regressor.fit(X_train, y_train)\n",
    "y_pred_linear = linear_regressor.predict(X_test)\n",
    "model_results['Linear Regression'] = {\n",
    "    'MSE': mean_squared_error(y_test, y_pred_linear),\n",
    "    'R2 Score': r2_score(y_test, y_pred_linear)\n",
    "}\n",
    "\n",
    "# 2. Ridge Regression\n",
    "ridge_regressor = Ridge(alpha=1.0)\n",
    "ridge_regressor.fit(X_train, y_train)\n",
    "y_pred_ridge = ridge_regressor.predict(X_test)\n",
    "model_results['Ridge Regression'] = {\n",
    "    'MSE': mean_squared_error(y_test, y_pred_ridge),\n",
    "    'R2 Score': r2_score(y_test, y_pred_ridge)\n",
    "}\n",
    "\n",
    "# 3. Lasso Regression\n",
    "lasso_regressor = Lasso(alpha=0.1)\n",
    "lasso_regressor.fit(X_train, y_train)\n",
    "y_pred_lasso = lasso_regressor.predict(X_test)\n",
    "model_results['Lasso Regression'] = {\n",
    "    'MSE': mean_squared_error(y_test, y_pred_lasso),\n",
    "    'R2 Score': r2_score(y_test, y_pred_lasso)\n",
    "}\n",
    "\n",
    "# 4. Decision Tree Regressor\n",
    "decision_tree = DecisionTreeRegressor()\n",
    "decision_tree.fit(X_train, y_train)\n",
    "y_pred_dt = decision_tree.predict(X_test)\n",
    "model_results['Decision Tree Regression'] = {\n",
    "    'MSE': mean_squared_error(y_test, y_pred_dt),\n",
    "    'R2 Score': r2_score(y_test, y_pred_dt)\n",
    "}\n",
    "\n",
    "# 5. Random Forest Regressor\n",
    "random_forest = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "random_forest.fit(X_train, y_train)\n",
    "y_pred_rf = random_forest.predict(X_test)\n",
    "model_results['Random Forest Regression'] = {\n",
    "    'MSE': mean_squared_error(y_test, y_pred_rf),\n",
    "    'R2 Score': r2_score(y_test, y_pred_rf)\n",
    "}\n",
    "\n",
    "# 6. Support Vector Regression (SVR)\n",
    "svr_regressor = SVR()\n",
    "svr_regressor.fit(X_train, y_train)\n",
    "y_pred_svr = svr_regressor.predict(X_test)\n",
    "model_results['Support Vector Regression'] = {\n",
    "    'MSE': mean_squared_error(y_test, y_pred_svr),\n",
    "    'R2 Score': r2_score(y_test, y_pred_svr)\n",
    "}\n",
    "\n",
    "# 7. K-Nearest Neighbors (KNN) Regression\n",
    "knn_regressor = KNeighborsRegressor(n_neighbors=5)\n",
    "knn_regressor.fit(X_train, y_train)\n",
    "y_pred_knn = knn_regressor.predict(X_test)\n",
    "model_results['K-Nearest Neighbors Regression'] = {\n",
    "    'MSE': mean_squared_error(y_test, y_pred_knn),\n",
    "    'R2 Score': r2_score(y_test, y_pred_knn)\n",
    "}\n",
    "\n",
    "# Display model evaluation results\n",
    "for model_name, metrics in model_results.items():\n",
    "    print(f\"{model_name}: MSE = {metrics['MSE']:.2f}, R2 Score = {metrics['R2 Score']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e4a697-835a-4229-bcff-492c31a590fe",
   "metadata": {},
   "source": [
    "## here also, when only numerical columns are used SVR seems to be the most promising model, followed by Lasso Regression because of low MSE and high R2 Score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226bdfd5-b205-4c41-843a-29ae7080d201",
   "metadata": {},
   "source": [
    "## **Predicting the feature importance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7033b0c-4bdd-4882-8f25-6d5e0a2c4462",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X_data = df.drop(columns='tip')\n",
    "y_target = df['tip']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train_data, X_test_data, y_train_data, y_test_data = train_test_split(X_data, y_target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_data)\n",
    "X_test_scaled = scaler.transform(X_test_data)\n",
    "\n",
    "# Fit a Linear Regression model\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train_scaled, y_train_data)\n",
    "\n",
    "# Extract coefficients and feature names\n",
    "coefficients = linear_model.coef_\n",
    "features = X_data.columns\n",
    "\n",
    "# Create a DataFrame to show feature importance\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Coefficient': coefficients\n",
    "})\n",
    "\n",
    "# Add a column for the absolute value of coefficients to measure importance\n",
    "importance_df['Abs_Coefficient'] = np.abs(importance_df['Coefficient'])\n",
    "importance_df = importance_df.sort_values(by='Abs_Coefficient', ascending=False)\n",
    "\n",
    "# Print feature importance based on the magnitude of the coefficients\n",
    "print(\"Feature Importance (based on coefficient magnitude):\")\n",
    "print(importance_df[['Feature', 'Coefficient']])\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Abs_Coefficient', y='Feature', data=importance_df)\n",
    "plt.title(\"Feature Importance Based on Linear Regression Coefficients\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8936ebc7-bc68-464d-be25-c43355747b42",
   "metadata": {},
   "source": [
    "## As we can see the total bill column is most important feature to predict the tip, thus we can separately use it to build a new model to improve the speed and accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d152ad-6ece-4d44-845d-d2bd5df3d982",
   "metadata": {},
   "source": [
    "## **Using only total bill column to predict the tip**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8eda3a-7bdb-4e76-9299-db4673b1f35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X_data = df.drop(columns='tip')\n",
    "y_target = df['tip']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train_data, X_test_data, y_train_data, y_test_data = train_test_split(X_data, y_target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_data)\n",
    "X_test_scaled = scaler.transform(X_test_data)\n",
    "\n",
    "# Fit a Linear Regression model\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train_scaled, y_train_data)\n",
    "\n",
    "# Extract coefficients and feature names\n",
    "coefficients = linear_model.coef_\n",
    "features = X_data.columns\n",
    "\n",
    "# Create a DataFrame to show feature importance\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Coefficient': coefficients\n",
    "})\n",
    "\n",
    "# Add a column for the absolute value of coefficients to measure importance\n",
    "importance_df['Abs_Coefficient'] = np.abs(importance_df['Coefficient'])\n",
    "importance_df = importance_df.sort_values(by='Abs_Coefficient', ascending=False)\n",
    "\n",
    "# Print feature importance based on the magnitude of the coefficients\n",
    "print(\"Feature Importance (based on coefficient magnitude):\")\n",
    "print(importance_df[['Feature', 'Coefficient']])\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Abs_Coefficient', y='Feature', data=importance_df)\n",
    "plt.title(\"Feature Importance Based on Linear Regression Coefficients\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d7b837-70a3-443a-9b53-6901010c5848",
   "metadata": {},
   "source": [
    "## **Model Performance with Only Total Bill Feature**\n",
    "\n",
    "When only the **Total Bill** column is used as a feature, both the **Mean Squared Error (MSE)** decreases and the **R-squared (RÂ²)** score increases. Among all the models tested, **Lasso Regression** performs the best in this scenario, demonstrating the most accurate predictions with the fewest features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f71a18e-8221-4a4d-b6de-d37a1aa947c6",
   "metadata": {},
   "source": [
    "## **Conclusion**\n",
    "\n",
    "### **Factors Affecting Tip Amounts:**\n",
    "- Several models were trained using different sets of features. The feature importance was assessed using regression algorithms. Among all the features, **Total Bill** had the strongest impact on the tip amount, followed by **Group Size** (the number of people dining together).\n",
    "\n",
    "### **Prediction Accuracy:**\n",
    "- **Support Vector Regression (SVR)** provided the highest R-squared value of 0.57 and consistently delivered the best results in terms of **Mean Squared Error (MSE)**.\n",
    "- **Lasso Regression** produced good results when only the **Total Bill** feature was used, indicating that it captures the relationship well with minimal variables.\n",
    "- The other models performed reasonably well; however, the **MSE** for all models did not drop below 0.30, suggesting that there is still room for improvement in the predictive accuracy.\n",
    "\n",
    "### **Insights for Management:**\n",
    "- The **Total Bill** has a significant influence on the tip amount. Therefore, encouraging **premium customers** to visit more frequently could lead to an increase in tips. This can be achieved by offering **special promotions**, such as free drinks or loyalty rewards.\n",
    "- Offering **premium dishes** of high quality at a moderate price point could also drive higher tip amounts, as these would likely increase the overall total bill, positively influencing the tip.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e3a347-81b4-4a7c-82b8-7166092807cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
